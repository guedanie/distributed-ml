{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes\n",
    "\n",
    "* Look like pandas dataframes\n",
    "* share some of the same methods and syntax\n",
    "* but they are 2 separate types of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crate Dataframes\n",
    "\n",
    "Convert pandas dataframe into a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n group\n",
       "0    0     b\n",
       "1    1     b\n",
       "2    2     c\n",
       "3    3     a\n",
       "4    4     c\n",
       "5    5     c\n",
       "6    6     a\n",
       "7    7     b\n",
       "8    8     a\n",
       "9    9     b\n",
       "10  10     b\n",
       "11  11     a\n",
       "12  12     b\n",
       "13  13     a\n",
       "14  14     b\n",
       "15  15     b\n",
       "16  16     c\n",
       "17  17     c\n",
       "18  18     a\n",
       "19  19     c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20))\n",
    ")\n",
    "pandas_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Spark Dataframe from Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: bigint, group: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_df = spark.createDataFrame(pandas_dataframe)\n",
    "sp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We do see the column names, but don't see the data\n",
    "* Because spark is lazy, in that it won't show us values until it has to\n",
    "* To peek, use `.show`\n",
    "    * `.show` defaults to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    b|\n",
      "|  1|    b|\n",
      "|  2|    c|\n",
      "|  3|    a|\n",
      "|  4|    c|\n",
      "|  5|    c|\n",
      "|  6|    a|\n",
      "|  7|    b|\n",
      "|  8|    a|\n",
      "|  9|    b|\n",
      "| 10|    b|\n",
      "| 11|    a|\n",
      "| 12|    b|\n",
      "| 13|    a|\n",
      "| 14|    b|\n",
      "| 15|    b|\n",
      "| 16|    c|\n",
      "| 17|    c|\n",
      "| 18|    a|\n",
      "| 19|    c|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read Data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "\n",
    "* The following will create a series from a pandas dataframe, but a column object from a spark dataframe. \n",
    "* A column object represents a vertical slice of a dataframe, but does not contain the data itself. \n",
    "* You will use it to perform function on and reference that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'year'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|hwy|cty|     model|\n",
      "+---+---+----------+\n",
      "| 29| 18|        a4|\n",
      "| 29| 21|        a4|\n",
      "| 31| 20|        a4|\n",
      "| 30| 21|        a4|\n",
      "| 26| 16|        a4|\n",
      "| 26| 18|        a4|\n",
      "| 27| 18|        a4|\n",
      "| 26| 18|a4 quattro|\n",
      "| 25| 16|a4 quattro|\n",
      "| 28| 20|a4 quattro|\n",
      "+---+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(hwy + 1)'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column objects support operations such as arithmetic operations\n",
    "\n",
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "+---+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy + 1).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|highway_mileage|hwy_miledge_plus1|\n",
      "+---------------+-----------------+\n",
      "|             29|               30|\n",
      "|             29|               30|\n",
      "|             31|               32|\n",
      "|             30|               31|\n",
      "|             26|               27|\n",
      "|             26|               27|\n",
      "|             27|               28|\n",
      "|             26|               27|\n",
      "|             25|               26|\n",
      "|             28|               29|\n",
      "|             27|               28|\n",
      "|             25|               26|\n",
      "|             25|               26|\n",
      "|             25|               26|\n",
      "|             25|               26|\n",
      "|             24|               25|\n",
      "|             25|               26|\n",
      "|             23|               24|\n",
      "|             20|               21|\n",
      "|             15|               16|\n",
      "+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can rename the columns as an alias\n",
    "\n",
    "mpg.select(mpg.hwy.alias(\"highway_mileage\"),\n",
    "           (mpg.hwy + 1).alias(\"hwy_miledge_plus1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "|             30|                  15.0|\n",
      "|             26|                  13.0|\n",
      "|             26|                  13.0|\n",
      "|             27|                  13.5|\n",
      "|             26|                  13.0|\n",
      "|             25|                  12.5|\n",
      "|             28|                  14.0|\n",
      "|             27|                  13.5|\n",
      "|             25|                  12.5|\n",
      "|             25|                  12.5|\n",
      "|             25|                  12.5|\n",
      "|             25|                  12.5|\n",
      "|             24|                  12.0|\n",
      "|             25|                  12.5|\n",
      "|             23|                  11.5|\n",
      "|             20|                  10.0|\n",
      "|             15|                   7.5|\n",
      "+---------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also store column objects in variables and reference them\n",
    "\n",
    "col1 = mpg.hwy.alias(\"highway_mileage\")\n",
    "col2 = (mpg.hwy / 2).alias(\"highway_mileage_halved\")\n",
    "mpg.select(col1, col2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the syntax we've seen above, we can create columns with the `col` and `expr` functions from `pyspark.sql.function` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "col(\"hwy\")\n",
    "# above is same as this  mpg.hwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+\n",
      "|highway_mileage|city_mileage|avg_mileage|\n",
      "+---------------+------------+-----------+\n",
      "|             29|          18|       23.5|\n",
      "|             29|          21|       25.0|\n",
      "|             31|          20|       25.5|\n",
      "|             30|          21|       25.5|\n",
      "|             26|          16|       21.0|\n",
      "+---------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_column = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "\n",
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"highway_mileage\"),\n",
    "    mpg.cty.alias(\"city_mileage\"),\n",
    "    avg_column.alias(\"avg_mileage\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a variable named avg_column that represents the average of the highway and city mileage of each vehicle. This variable is created by using the col function to produce pyspark Column objects and using the arithmetic operators to combine them.\n",
    "\n",
    "Next we select the original highway and city mileage columns, in addition to our new average mileage column. We demonstrate the col function to select the hwy column and refer to the city mileage column with the df.cty syntax we saw previously. We also give all of our columns more readable aliases before showing the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expr\n",
    "\n",
    "The expr function is more powerful than col. It does everything col does and more. expr returns the same type of column object, but allows us to express manipulations to the column within the string that defines the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-------------------+\n",
      "|hwy|(hwy + 1)|highway_mileage|highway_incremented|\n",
      "+---+---------+---------------+-------------------+\n",
      "| 29|       30|             29|                 30|\n",
      "| 29|       30|             29|                 30|\n",
      "| 31|       32|             31|                 32|\n",
      "| 30|       31|             30|                 31|\n",
      "| 26|       27|             26|                 27|\n",
      "+---+---------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    expr(\"hwy\"),  # the same as `col`\n",
    "    expr(\"hwy + 1\"),  # an arithmetic expression\n",
    "    expr(\"hwy AS highway_mileage\"),  # using an alias\n",
    "    expr(\"hwy + 1 AS highway_incremented\"),  # a combination of the above\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "|     30|     30|     30|     30|\n",
      "|     26|     26|     26|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that all the columns created below are identical,\n",
    "# and which syntax to use is merely a style choice.\n",
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "* Spark SQL allows us to write SQL queries against our spark dataframes\n",
    "* We'll first \"register\" the table with staprk with `sp_df.createOrReplaceTempView(\"sp_df\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, avg: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can write a sql query against the `mpg` table\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting value is another dataframe. As we know, in order to view the values in a dataframe, we need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "| 31| 20|25.5|\n",
      "| 30| 21|25.5|\n",
      "| 26| 16|21.0|\n",
      "+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting\n",
    "\n",
    "View column datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To convert the data types we use .cast\n",
    "\n",
    "mpg.select(mpg.hwy.cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Built-in Functions\n",
    "\n",
    "There are many other functions beyond col and expr within the `pyspark.sql.functions` module for operating on pyspark dataframe columns. \n",
    "\n",
    "* `concat`: to concatenate string\n",
    "* `sum`: to sum a group\n",
    "* `avg`: to take the average of a group\n",
    "* `min`: to find the minimun\n",
    "* `max`: to find the max\n",
    "\n",
    "**Note**: Importing the sum, min, and max functions directly will override the build-in sum, min and max functions. This means you will get an error if you try to sum a list of numbers, because sum will refrence the relative pyspark function, which works with pyspark dataframe columns, while the relative built-in function works with lists of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+--------+--------+\n",
      "|        average_1|        average_2|min(hwy)|max(hwy)|\n",
      "+-----------------+-----------------+--------+--------+\n",
      "|23.44017094017094|23.44017094017094|      12|      44|\n",
      "+-----------------+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    (sum(mpg.hwy) / count(mpg.hwy)).alias(\"average_1\"),\n",
    "    avg(mpg.hwy).alias(\"average_2\"),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a string literaly as part of our select, we'll need to use the `lit` function, otherwise spark will try to resolve our string as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|concat(cyl,  cylinders)|\n",
      "+-----------------------+\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            6 cylinders|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "mpg.select(concat(mpg.cyl, lit(\" cylinders\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `pyspark` Functions for String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_extract`: Specifiy a regular expression with at least one capture group, and create a new column based on the contents of a capture group.\n",
    "\n",
    "* FIrst argumetn: the name of the string column to extract from\n",
    "* second argument: the regular expression itself\n",
    "* last argument: specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it, and we wanted the contents of the 2nd group, we would sepecigy a 2 here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_extract(\"address\", r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "    regexp_extract(\"address\", r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the first argument to regexp_extract is the name of the string column to extract from, the second argument is the regular expression itself, and the last argument specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here.\n",
    "\n",
    "In addition to `regexp_extract`, `regexp_replace` lets us make substitutions based on a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.filter` and `.where`\n",
    "\n",
    "Sprak provides two dataframe methods, `.filter` and `.where`, wich bot hallow us to select a subset of the rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|      model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 25| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 23| 29|  p|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|manual(m5)|  f| 26| 34|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "|       honda|      civic|  2.0|2008|  4|manual(m6)|  f| 21| 29|  p|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|  auto(l4)|  f| 19| 26|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|manual(m5)|  f| 19| 29|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|manual(m5)|  f| 20| 28|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|  auto(l4)|  f| 20| 27|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|  auto(l4)|  4| 21| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|  auto(l4)|  4| 19| 26|  r|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|  volkswagen| new beetle|  2.0|1999|  4|manual(m5)|  f| 21| 29|  r|subcompact|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 4).where(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When and Otherwise\n",
    "\n",
    "Similar to an `IF` in excel, `CASE...WHEN` in SQL, or `np.where` in python, sprak provides a `when` function. \n",
    "\n",
    "The `when` function lets us specify a condition, and a value to produce if that condition is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|    mpg_desc|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25|        null|\n",
      "| 28|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 25|        null|\n",
      "+---+------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, when(mpg.hwy > 25, \"good_mileage\").alias(\"mpg_desc\")).show(\n",
    "    12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that if the condition we specified is false, `null` will be produced. Instead of null, we can specify a value to use if our condition is false with the `.otherwise` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|    mpg_desc|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25| bad_mileage|\n",
      "| 28|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 25| bad_mileage|\n",
      "+---+------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy,\n",
    "    when(mpg.hwy > 25, \"good_mileage\")\n",
    "    .otherwise(\"bad_mileage\")\n",
    "    .alias(\"mpg_desc\"),\n",
    ").show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify multiple conditions, we can chain `.when` calls. The first condition that is met will be the value that is used, and if none of the conditions are met the value specified in the `.otherwise` will be used (or null if you don't provide a `.otherwise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|displ|engine_size|\n",
      "+-----+-----------+\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "|  2.0|     medium|\n",
      "|  2.8|     medium|\n",
      "|  2.8|     medium|\n",
      "|  3.1|      large|\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (\n",
    "        when(mpg.displ < 2, \"small\")\n",
    "        .when(mpg.displ < 3, \"medium\")\n",
    "        .otherwise(\"large\")\n",
    "        .alias(\"engine_size\")\n",
    "    ),\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that a car with a `displ` of 1.8 matches both conditions we specified, but small is produced because it is associated with the first matching condition. For any value between 2 and 3, medium will be produced, and anything larger than 3 will produce large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "Spark lets us sort the rows in our dataframe by one or multiple columns with two methods: `.sort`, and `.orderby`. `.sort` and `.orderby` are aliases of each other and do the exact same thing. Like other methods we've seen, `.sort` takes in a column object or a string that is the name of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|manufacturer|              model|displ|year|cyl|     trans|drv|cty|hwy| fl| class|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|        jeep| grand cherokee 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|manual(m6)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|        durango 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|  dakota pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|        jeep| grand cherokee 4wd|  6.1|2008|  8|  auto(l5)|  4| 11| 14|  p|   suv|\n",
      "|   chevrolet|    k1500 tahoe 4wd|  5.3|2008|  8|  auto(l4)|  4| 11| 14|  e|   suv|\n",
      "|  land rover|        range rover|  4.0|1999|  8|  auto(l4)|  4| 11| 15|  p|   suv|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, values are sorted in ascending order. To sort in descending order, we can use the `.desc` method on any Column object, or the `desc`function from `pyspark.sql.functions.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy.desc())\n",
    "# is the same as\n",
    "mpg.sort(col(\"hwy\").desc())\n",
    "# is the same as\n",
    "mpg.sort(desc(\"hwy\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify sorting by multiple columns, we provide each column as a separate argument to `.sort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 20| 27|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 20| 26|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|manual(m5)|  4| 18| 25|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 19| 25|  p|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|  auto(l4)|  4| 18| 24|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 18| 23|  p|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|manual(m5)|  4| 15| 20|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|  auto(l4)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.0|2008|  6|  auto(l5)|  4| 17| 22|  d|  suv|\n",
      "|      nissan|    pathfinder 4wd|  4.0|2008|  6|  auto(l5)|  4| 14| 20|  p|  suv|\n",
      "|      toyota|       4runner 4wd|  4.0|2008|  6|  auto(l5)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  4.0|1999|  6|  auto(l4)|  4| 15| 20|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|manual(m5)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.7|2008|  6|  auto(l5)|  4| 15| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  3.4|1999|  6|  auto(l4)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(desc(\"class\"), mpg.cyl.asc(), col(\"hwy\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating\n",
    "\n",
    "To aggregate our data by group, we can use the `.groupBy` method. Like with `.select`, we can pass either Column objects or strings that are column names to `.groupBy`. All of the expressions below are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11b0a94d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(col(\"cyl\"))\n",
    "mpg.groupBy(\"cyl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is grouped, we need to specify an aggregation. We can use one of the aggregate functions we imported earlier, alond with a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|cyl|          avg(cty)|         avg(hwy)|\n",
      "+---+------------------+-----------------+\n",
      "|  6| 16.21518987341772|22.82278481012658|\n",
      "|  5|              20.5|            28.75|\n",
      "|  8|12.571428571428571|17.62857142857143|\n",
      "|  4|21.012345679012345|28.80246913580247|\n",
      "+---+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group by multiple columns, pass each of the columns a a separate argument to `.groupBy` (Note that this is different from pandas, where we would need to pass a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "|cyl|     class|          avg(cty)|          avg(hwy)|\n",
      "+---+----------+------------------+------------------+\n",
      "|  5|   compact|              21.0|              29.0|\n",
      "|  5|subcompact|              20.0|              28.5|\n",
      "|  6|subcompact|              17.0|24.714285714285715|\n",
      "|  6|    pickup|              14.5|              17.9|\n",
      "|  4|subcompact|22.857142857142858| 30.80952380952381|\n",
      "|  8|       suv|12.131578947368421|16.789473684210527|\n",
      "|  8|    pickup|              11.8|              15.8|\n",
      "|  8|   midsize|              16.0|              24.0|\n",
      "|  4|   midsize|              20.5|           29.1875|\n",
      "|  8|   2seater|              15.4|              24.8|\n",
      "|  6|   compact|16.923076923076923|25.307692307692307|\n",
      "|  6|   minivan|              15.6|              22.2|\n",
      "|  4|   compact|            21.375|          29.46875|\n",
      "|  8|subcompact|              14.8|              21.6|\n",
      "|  6|   midsize|17.782608695652176| 26.26086956521739|\n",
      "|  4|   minivan|              18.0|              24.0|\n",
      "|  4|    pickup|              16.0|20.666666666666668|\n",
      "|  6|       suv|              14.5|              18.5|\n",
      "|  4|       suv|              18.0|             23.75|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(\"cyl\", \"class\").agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `.groupBy`, we can use `.rollup`, which will do the same aggregations, but will also include the overall total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| cyl|count|\n",
      "+----+-----+\n",
      "|null|  234|\n",
      "|   4|   81|\n",
      "|   5|    4|\n",
      "|   6|   79|\n",
      "|   8|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").count().sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the null value in cyl indicates the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| cyl|         avg(hwy)|\n",
      "+----+-----------------+\n",
      "|null|23.44017094017094|\n",
      "|   4|28.80246913580247|\n",
      "|   5|            28.75|\n",
      "|   6|22.82278481012658|\n",
      "|   8|17.62857142857143|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").agg(expr(\"avg(hwy)\")).sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the example above, the null row represents the overall average highway mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     class|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|null|      null| 23.44017094017094|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|   compact|          29.46875|\n",
      "|   4|   midsize|           29.1875|\n",
      "|   4|   minivan|              24.0|\n",
      "|   4|    pickup|20.666666666666668|\n",
      "|   4|subcompact| 30.80952380952381|\n",
      "|   4|       suv|             23.75|\n",
      "|   5|      null|             28.75|\n",
      "|   5|   compact|              29.0|\n",
      "|   5|subcompact|              28.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   6|   compact|25.307692307692307|\n",
      "|   6|   midsize| 26.26086956521739|\n",
      "|   6|   minivan|              22.2|\n",
      "|   6|    pickup|              17.9|\n",
      "|   6|subcompact|24.714285714285715|\n",
      "|   6|       suv|              18.5|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   8|   2seater|              24.8|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\", \"class\").mean(\"hwy\").sort(col(\"cyl\"), col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstabs and Pivot Tables\n",
    "\n",
    "In addition to groupby, spark provides a couple other ways to do aggregation. One of which is `.crosstab`. This is very similary to pandas `.crosstab` function, in that it calculates the number of occurances of each unique value from the two passed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+\n",
      "| class_cyl|  4|  5|  6|  8|\n",
      "+----------+---+---+---+---+\n",
      "|   midsize| 16|  0| 23|  2|\n",
      "|subcompact| 21|  2|  7|  5|\n",
      "|   2seater|  0|  0|  0|  5|\n",
      "|    pickup|  3|  0| 10| 20|\n",
      "|   minivan|  1|  0| 10|  0|\n",
      "|       suv|  8|  0| 16| 38|\n",
      "|   compact| 32|  2| 13|  0|\n",
      "+----------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.crosstab(\"class\", \"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.crosstab` simply does counts, if we want a different aggregation, we can use `.pivot`. For example, to find the average highway mileage for each combination of car class and number of cylinders, we could write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupby(\"class\").pivot(\"cyl\").mean(\"hwy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the unique values from the column we group by will be the rows in the resulting dataframe, and the unique values from the column we pivot on will become the columns. The values in each cell will be equal to the aggregation we specified over the group of values defined by the intersection of the rows and the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Let's take a look at how sprak handles missing data. First we'll create a dataframe that has a few missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two main ways to deal with missing values:\n",
    "\n",
    "* `.fill`: to replace missing values with a specified value\n",
    "* `.drop`: to drop rows containing missing values\n",
    "\n",
    "\n",
    "Both methods are accessed through the `.na` property. We'll look at some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both methods, we can sepcify that we only want to fill or drop values in a specific column with a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset=\"x\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above the na values in the x column were filled with 0, but the na values in y were left alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the rows that had an na value for the y column were dropped, but the rows with na values for only the x column are still present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining DataFrame Transformations\n",
    "\n",
    "The `.explain` method will show us how spark is thinking about our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our basic example, we see that there is only a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#19L, hwy#23L]\n",
      "+- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that there is another step after \"Scan ExistingRDD\", a \"Project\" that contains the names of the columns we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(cast((cyl#19L + hwy#23L) as double) / 2.0) AS avg_mpg#1377]\n",
      "+- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.select(((mpg.cyl + mpg.hwy) / 2).alias(\"avg_mpg\")).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are doing a more advanced select calculation, but this is still just a single step to spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(cyl#19L) && (cyl#19L = 6))\n",
      "+- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our filter is also a single step.\n",
    "\n",
    "Without reading ahead, do you think the execution plan for the two dataframes below will be the same or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#19L, hwy#23L]\n",
      "+- *(1) Filter (isnotnull(cyl#19L) && (cyl#19L = 6))\n",
      "   +- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n",
      "== Physical Plan ==\n",
      "*(1) Project [cyl#19L, hwy#23L]\n",
      "+- *(1) Filter (isnotnull(cyl#19L) && (cyl#19L = 6))\n",
      "   +- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\"cyl\", \"hwy\").filter(expr(\"cyl = 6\")).explain()\n",
    "mpg.filter(expr(\"cyl = 6\")).select(\"cyl\", \"hwy\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though we specified the transformations (`.select` and `.filter`) in a different order, we end up with the same output when we call `.explain`. This is because spark will look at our dataframe and transform it into the most efficient representation possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [hwy#23L]\n",
      "+- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.selectExpr(\"cyl + 3 * 16 / 4 + 19 AS unused\", \"hwy\").select(\n",
    "    \"hwy\"\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that we have 2 seperate select statements, but spark will condense this down to a single Project, as it is smart enough to realize that it doesn't actually need to do all the arithmetic we specified in the first select, since we arent using that value later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[min(cyl#19L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_min(cyl#19L)])\n",
      "      +- *(1) Project [cyl#19L]\n",
      "         +- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.select(min(mpg.cyl)).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that the execution plan gets much more complicated. This is because in steps prior, we were applying transformations that applied to each row individually. To calculate a minimum, we have to look at all the rows in the dataset to find the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#19L], functions=[min(hwy#23L), max(hwy#23L)])\n",
      "+- Exchange hashpartitioning(cyl#19L, 200)\n",
      "   +- *(1) HashAggregate(keys=[cyl#19L], functions=[partial_min(hwy#23L), partial_max(hwy#23L)])\n",
      "      +- *(1) Project [cyl#19L, hwy#23L]\n",
      "         +- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "mpg.groupby(mpg.cyl).agg(min(mpg.hwy), max(mpg.hwy)).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#19L], functions=[min(avg_mpg#1414), avg(avg_mpg#1414), max(avg_mpg#1414)])\n",
      "+- Exchange hashpartitioning(cyl#19L, 200)\n",
      "   +- *(1) HashAggregate(keys=[cyl#19L], functions=[partial_min(avg_mpg#1414), partial_avg(avg_mpg#1414), partial_max(avg_mpg#1414)])\n",
      "      +- *(1) Project [cyl#19L, (cast((cty#22L + hwy#23L) as double) / 2.0) AS avg_mpg#1414]\n",
      "         +- *(1) Filter (isnotnull(class#25) && (class#25 = compact))\n",
      "            +- Scan ExistingRDD[manufacturer#15,model#16,displ#17,year#18L,cyl#19L,trans#20,drv#21,cty#22L,hwy#23L,fl#24,class#25]\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    mpg.select(col(\"cyl\"), expr(\"(cty + hwy) / 2 AS avg_mpg\"))\n",
    "    .filter(expr('class == \"compact\"'))\n",
    "    .groupby(\"cyl\")\n",
    "    .agg(min(\"avg_mpg\"), avg(\"avg_mpg\"), max(\"avg_mpg\"))\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Manipulation Examples\n",
    "\n",
    "Let's take a look at some more examples of working with spark dataframes. For these examples, we'll be working with a dataset of observations of the weather in seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the number of rows and columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461 rows 6 columns\n"
     ]
    }
   ],
   "source": [
    "print(weather.count(), \"rows\", len(weather.columns), \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2012-01-01', '2015-12-31')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
