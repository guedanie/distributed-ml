{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "In this lesson, we will acquire and prepare the data we will use in the rest of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "Spark lets us read data in from a variety of data sources using what it calls a `DataFrameReader`. We can access the `read` property of our `spark` object and then set various options and read from a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"data/source.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code and also be written as\n",
    "\n",
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Schemas\n",
    "\n",
    "Spark incldues a concept of a *data schema*, which is a way to specify the types of our data ahead of time. Doing so lets us be sure about the structure of our data, and can significantly increase the speed of loading data (inferring the schema can be a costly operation for a large datasets).\n",
    "\n",
    "We'll import several things from the `pyspark.sql.types` module:\n",
    "\n",
    "* `StringType`\n",
    "* `DoubleType`\n",
    "* `IntegerType`\n",
    "* `LongType`\n",
    "* `ShortType`\n",
    "* `TimestampType`\n",
    "* `FloatType`\n",
    "* `DataType`\n",
    "\n",
    "All of the above types will go inside of a `StructField`, which wil be encaptsulated in a `StructType`, and the resulting object will represent our data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"source_id\", StringType()),\n",
    "#         StructField(\"source_username\", StringType()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# spark.read.csv(\"data/source.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "A spark dataframe can be written to a local destination using the `.write` property. Serveral common output formats are:\n",
    "\n",
    "* `csv`: for writing to a local csv file\n",
    "* `parquet`: Parquet is a very populat columnar storage format for Hadoop\n",
    "* `json`: for writing to a local json file(s)\n",
    "* `jdbc`: for writing to a SQL databse table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for demo purposes\n",
    "# from pydataset import data\n",
    "\n",
    "# mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "\n",
    "# mpg.write.json(\"data/mpg_json\", mode=\"overwrite\")\n",
    "\n",
    "# # like much else in spark, there's multiple ways we could do this:\n",
    "# (\n",
    "#     mpg.write.format(\"csv\")\n",
    "#     .mode(\"overwrite\")\n",
    "#     .option(\"header\", \"true\")\n",
    "#     .save(\"data/mpg_csv\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For the rest of this lession, we'll take a look at the `case` data from the San Antionio 311 calls dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"data/case.csv\", header=True, inferSchema=True)\n",
    "# df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns\n",
    "\n",
    "We'll rename this column to match with the other date-type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRename(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Data Types\n",
    "\n",
    "Two columns, `case_closed` and `case_late` store yes/no values. Currently spark things they are strings; let's turn them into booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"case_closed\", expr('case_closed == \"YES\"')).withColumn(\n",
    "    \"case_late\", expr('case_late == \"YES\"')\n",
    ")\n",
    "\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `council_district` column appears as though it is an integer, but this is just a unique identifier for each district, that is, we aren't going to be performing arithmetic with this number, so we will turn it into a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will handle the 3 columns that have dates in them. We'll use spark's `to_timestamp` function for this.\n",
    "\n",
    "In order to work properly, we'll need to provide the date format when using `to_timestamp`. The date format is a little different than the date functionality we've worked with in pandas, this is becuase it is using Java's SimpleDateFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Before handling dates\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)\n",
    "\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "df = (\n",
    "    df.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_due_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    ")\n",
    "\n",
    "print(\"--- After\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "\n",
    "Now that we have everything stored as the correct data type, we will make a few transformations to the data.\n",
    "\n",
    "We'll begin by normalizing the request address field. Using the `trim` and `lower` functions lets us strip any leading or trainling whitespace and converty everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Before\")\n",
    "df.select(\"request_address\").show(5)\n",
    "\n",
    "df = df.withColumn(\"request_address\", trim(lower(df.request_address)))\n",
    "\n",
    "print(\"--- After\")\n",
    "df.select(\"request_address\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the number of days a case is late to a number of weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"num_weeks_late\", expr(\"num_days_late / 7 AS num_weeks_late\")\n",
    ")\n",
    "\n",
    "df.select(\"num_days_late\", \"num_weeks_late\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can format the council district column a little differently. We'll add leading 0s to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"int\"))\n",
    "\n",
    "# '%03d' means at least 3 digits, pad with 0s\n",
    "#\n",
    "# In order to use the format_string function the way we are, we'll need to\n",
    "# convert council_district back to an integer temporarily, but the final output\n",
    "# will be a string.\n",
    "df = df.withColumn(\n",
    "    \"council_district\",\n",
    "    format_string(\"%03d\", col(\"council_district\").cast(\"int\")),\n",
    ")\n",
    "\n",
    "df.select(\"council_district\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features\n",
    "\n",
    "Let's now create some new features basd on our existing data.\n",
    "\n",
    "We will first extract the zipcode from the address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "\n",
    "df.select(\"zipcode\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined the zipcode as the last sequence of digits at the end of the string.\n",
    "\n",
    "Next we will create several new, related columns:\n",
    "\n",
    "* `case_age`: How old the case is; the difference in days between when the case was opened and the current day\n",
    "* `days_to_closed`: The number of days between when the case was opened and when it was closed\n",
    "* `case_lifetime`: Number of days between when the case was opened and when it was closed, if the case is still open, the number of days since the case was opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"case_closed\")).show(5)\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"! case_closed\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Department Data\n",
    "\n",
    "We have access to another dataset, `dept.csv`, that contains more information about the vaiours different departments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = spark.read.csv(\"data/dept.csv\", header=True, inferSchema=True)\n",
    "dept.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to include this data, so we can join it to our case dataframe using the `dept_division` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    # left join on dept_division\n",
    "    .join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .drop(df.dept_division)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")\n",
    "\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2])\n",
    "#\n",
    "train, validate, test = df.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
